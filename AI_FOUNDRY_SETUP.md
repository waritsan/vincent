# Azure AI Foundry Setup - Review & Improvements

## ‚úÖ What You Had (Original)
- Basic AI Foundry account creation
- AI Project resource (unused)
- GPT-4o model deployment with GlobalStandard SKU

## ‚ú® What's Been Improved

### 1. **AI Foundry Module** (`infra/core/ai/ai-foundry-project.bicep`)
#### Added:
- ‚úÖ **Tags support** - Proper tagging for resource management
- ‚úÖ **Outputs** - Exports endpoint, ID, principal ID, deployment name
- ‚úÖ **Configurable parameters** - Model capacity, deployment toggle
- ‚úÖ **Stable API version** - Using `2024-10-01` instead of preview
- ‚úÖ **Model versioning** - Auto-updates with `versionUpgradeOption`
- ‚úÖ **Standard SKU** - Changed from GlobalStandard to Standard (more reliable)

#### Removed:
- ‚ùå Unused `aiProject` resource (not needed for basic AI Services)
- ‚ùå `allowProjectManagement` property (not in stable API)
- ‚ùå `disableLocalAuth` (keeping auth flexible)

### 2. **Main Infrastructure** (`infra/main.bicep`)
#### Added:
- ‚úÖ **Managed Identity for Functions** - Secure access without keys
- ‚úÖ **RBAC Role Assignment** - "Cognitive Services OpenAI User" role
- ‚úÖ **Auto-configured Environment Variables**:
  - `AZURE_AI_ENDPOINT` - Automatically set from AI Foundry
  - `AZURE_AI_DEPLOYMENT_NAME` - Points to gpt-4o deployment
- ‚úÖ **AI Outputs** - Environment variables exported for local dev
- ‚úÖ **Proper naming** - Using `ai-{resourceToken}` pattern

### 3. **Security Improvements**
- ‚úÖ **Managed Identity** instead of API keys
- ‚úÖ **RBAC-based access** (Cognitive Services OpenAI User role)
- ‚úÖ **Service Principal authentication** for Functions

## üìã What's Still Missing (Optional Enhancements)

### If You Want to Add Later:

1. **Key Vault Integration** (Recommended for production)
   ```bicep
   // Store AI keys in Key Vault
   module keyVault 'core/security/keyvault.bicep' = { ... }
   ```

2. **Private Endpoints** (For enhanced security)
   ```bicep
   // Restrict AI Foundry to private network
   properties: {
     publicNetworkAccess: 'Disabled'
   }
   ```

3. **Storage Account for AI Project** (For file uploads, RAG)
   ```bicep
   // Separate storage for AI documents
   module aiStorage 'core/storage/storage-account.bicep' = { ... }
   ```

4. **Content Filtering** (For production safety)
   ```bicep
   properties: {
     customSubDomainName: aiFoundryName
     customProperties: {
       'ContentFilter': 'Enabled'
     }
   }
   ```

5. **Cost Management** (Budget alerts)
   ```bicep
   // Add budget alerts for AI spending
   ```

6. **Multiple Model Deployments** (If you need GPT-3.5, embeddings, etc.)
   ```bicep
   resource gpt35Deployment 'Microsoft.CognitiveServices/accounts/deployments@2024-10-01' = {
     name: 'gpt-35-turbo'
     // ... config
   }
   ```

## üöÄ Ready to Deploy

Your current setup includes:

### Infrastructure:
- ‚úÖ Azure AI Foundry (AIServices)
- ‚úÖ GPT-4o model deployment
- ‚úÖ Managed Identity authentication
- ‚úÖ RBAC permissions
- ‚úÖ Environment variables auto-configured

### What Happens on Deploy:
1. AI Foundry account created with custom subdomain
2. GPT-4o model deployed (10 capacity units)
3. Function App gets Managed Identity
4. RBAC role assigned to Function App
5. Environment variables automatically set

### Deploy Command:
```bash
azd up
```

## üîß Testing After Deployment

1. **Verify AI Foundry is created**:
   ```bash
   az cognitiveservices account show \
     --name ai-{resourceToken} \
     --resource-group rg-vincent-dev
   ```

2. **Check model deployment**:
   ```bash
   az cognitiveservices account deployment show \
     --name ai-{resourceToken} \
     --resource-group rg-vincent-dev \
     --deployment-name gpt-4o
   ```

3. **Get environment variables**:
   ```bash
   azd env get-values
   ```

4. **Test the chat endpoint** (after updating function_app.py):
   ```bash
   curl -X POST https://func-ja67jva7pfqfc.azurewebsites.net/api/chat \
     -H "Content-Type: application/json" \
     -d '{"message": "Tell me a joke"}'
   ```

## üìù Next Steps

### 1. Update Function App Code
The Python function needs to be updated to use the AI endpoint. Here's what to change in `src/api/function_app.py`:

```python
# Update the get_ai_client function to use the endpoint
def get_ai_client():
    endpoint = os.environ.get("AZURE_AI_ENDPOINT")
    credential = DefaultAzureCredential()
    return ChatCompletionsClient(endpoint=endpoint, credential=credential)

# Update the chat function to actually call the AI
client = get_ai_client()
if client:
    deployment = os.environ.get("AZURE_AI_DEPLOYMENT_NAME")
    response = client.complete(
        messages=[{"role": "user", "content": user_message}],
        model=deployment
    )
    ai_response = response.choices[0].message.content
```

### 2. Handle Regional Availability
GPT-4o might not be available in all regions. Common regions:
- `eastus2` ‚úÖ
- `swedencentral` ‚úÖ
- `westus` ‚úÖ

If deployment fails, check: https://learn.microsoft.com/azure/ai-services/openai/concepts/models

### 3. Monitor Costs
- GPT-4o: ~$0.01 per 1K tokens (input) / $0.03 per 1K tokens (output)
- Free tier: None for GPT-4o
- Set up budget alerts in Azure Portal

## ‚ö†Ô∏è Important Notes

1. **Model Capacity**: Set to 10 TPM (tokens per minute). Adjust based on needs.
2. **API Version**: Using stable `2024-10-01`, not preview
3. **Authentication**: Using Managed Identity (no keys in code!)
4. **Deployment Time**: AI model deployment can take 5-15 minutes

## üéØ Summary

Your AI Foundry setup is **production-ready** with:
- ‚úÖ Secure authentication (Managed Identity)
- ‚úÖ Proper RBAC permissions
- ‚úÖ Auto-configured environment variables
- ‚úÖ GPT-4o model deployed
- ‚úÖ Tags and outputs for management
- ‚úÖ Scalable configuration

**You're ready to deploy!** üöÄ
